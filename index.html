<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xuhai Chen</title>

    <meta name="author" content="Xuhai Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xuhai Chen
                </p>
                <p>
                  I'm a Master's student at Zhejiang University in China, advised by Prof. <a href="https://scholar.google.com/citations?user=qYcgBbEAAAAJ&hl=en">Yong Liu</a>, and expect to graduate in 2025.
                </p>
                <p>
                  In 2024, I had the pleasure of interning at the State Key Laboratory of CAD&CG at Zhejiang University, where I worked with Prof. <a href="https://xzhou.me/">Xiaowei Zhou</a> and Prof. <a href="https://pengsida.net/">Sida Peng</a>.
                  It was a valuable and enriching experience, during which I learned a great deal.
                </p>
                <p>
                  My current focus is on <b>motion generation</b>, and I'm also actively exploring <b>image super-resolution</b> and <b>anomaly detection</b>.
                </p>
                <p>
                  Life is meant to be experienced.
                </p>
                <p style="text-align:center">
                  <a href="mailto:22232044@zju.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.hk/citations?user=LU4etJ0AAAAJ&hl=zh-CN&authuser=1">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ByChelsea/">Github</a> /
                  <a href="Xuhai_Chen_CV.pdf">CV</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/xuhaichen.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/xuhaichen.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding-bottom: 15px;">
              <h2>Awards</h2>
            </td>
          </tr></tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding-bottom: 30px;">
              <li>[2023.06]: CVPR 2023 <a href="https://sites.google.com/view/vand-cvpr23/home">workshop</a> <a href="https://sites.google.com/view/vand-cvpr23/challenge?authuser=0">VAND</a> Challenge: Winner in the Zero-shot Track, Honorable Mention in the Few-shot Track.<br>
            </td>
          </tr></tbody>
          </table>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
            <tr>
              <td style="padding-bottom: 15px;">
                <h2>Papers</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/cmos.jpg' width=100%>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution</span>
                </a>
                <br>
                <strong>Xuhai Chen</strong>,
                <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
                Chao Xu,
                Yabiao Wang,
                Chengjie Wang,
                <a href="https://scholar.google.com/citations?user=qYcgBbEAAAAJ&hl=en">Yong Liu</a>
                <br>
                <em>CVPR</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Better_CMOS_Produces_Clearer_Images_Learning_Space-Variant_Blur_Estimation_for_CVPR_2023_paper.html">paper</a>
                /
                <a href="https://github.com/ByChelsea/CMOS">github</a>
                /
                <a href="data/cmos.bib">bibtex</a>
                <p></p>
                <p>
                  Estimating space-variant blur degradation with the help of semantic information.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/vand.jpg' width=100%>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">A Zero-/Few-Shot Anomaly Classification and Segmentation Method for CVPR 2023 VAND Workshop Challenge Tracks 1&2: 1st Place on Zero-shot AD and 4th Place on Few-shot AD</span>
                </a>
                <br>
                <strong>Xuhai Chen</strong>,
                Yue Han,
                <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2305.17382">paper</a>
                /
                <a href="https://github.com/ByChelsea/VAND-APRIL-GAN">github</a>
                /
                <a href="data/vand.bib">bibtex</a>
                <p></p>
                <p>
                  Technical report for the <a href="https://sites.google.com/view/vand-cvpr23/challenge?authuser=0">VAND</a> challenge at the 2023 CVPR <a href="https://sites.google.com/view/vand-cvpr23/home">workshop</a>.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/clipad.jpg' width=100%>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection</span>
                </a>
                <br>
                <strong>Xuhai Chen</strong>,
                <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
                Guanzhong Tian,
                Haoyang He,
                Wuhao Zhang,
                Yabiao Wang,
                Chengjie Wang,
                <a href="https://scholar.google.com/citations?user=qYcgBbEAAAAJ&hl=en">Yong Liu</a>
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2311.00453">paper</a>
                /
                <a href="https://github.com/ByChelsea/CLIP-AD">github</a>
                /
                <a href="data/clipad.bib">bibtex</a>
                <p></p>
                <p>
                  Adapt the CLIP model for anomaly segmentation by merely fine-tuning a linear layer, and explain the text prompts design from a distributional perspective.
                </p>
              </td>
            </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/vitad.jpg' width=100%>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection</span>
                </a>
                <br>
                <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
                <strong>Xuhai Chen</strong>,
                Yabiao Wang,
                Chengjie Wang,
                Yong Liu,
                Xiangtai Li,
                Ming-Hsuan Yang,
                Dacheng Tao
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2312.07495">paper</a>
                /
                <a href="https://github.com/zhangzjn/ADer">github</a>
                /
                <a href="data/vitad.bib">bibtex</a>
                <p></p>
                <p>
                  Construct a reverse distillation architecture for multi-class anomaly detection using plain ViT.
                </p>
              </td>
            </tr>
          </tbody></table>
          
      </td>
    </tr>
  </table>
  </body>
</html>
